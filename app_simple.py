#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FaceMood AI - Simple - DetecciÃ³n de emociones en tiempo real
===========================================================

VersiÃ³n simplificada que funciona realmente en tiempo real.
"""

import streamlit as st
import cv2
import numpy as np
import time
from datetime import datetime
import tempfile
import os
from deepface import DeepFace
import plotly.express as px
from collections import defaultdict
import threading

# ConfiguraciÃ³n de la pÃ¡gina
st.set_page_config(
    page_title="FaceMood AI - Simple",
    page_icon="ğŸ§ ",
    layout="wide"
)

def analyze_frame(frame):
    """Analiza un frame y devuelve los resultados."""
    try:
        # Crear archivo temporal
        temp_filename = f"temp_frame_{int(time.time() * 1000)}.jpg"
        temp_path = os.path.join(tempfile.gettempdir(), temp_filename)
        
        # Guardar frame
        cv2.imwrite(temp_path, frame)
        
        # Analizar con DeepFace
        result = DeepFace.analyze(
            temp_path, 
            actions=['emotion', 'age', 'gender'],
            enforce_detection=False
        )
        
        if result and len(result) > 0:
            analysis = result[0]
            
            # Extraer datos
            emotions = analysis.get('emotion', {})
            dominant_emotion = max(emotions, key=emotions.get) if emotions else 'neutral'
            emotion_confidence = emotions.get(dominant_emotion, 0) / 100.0
            age = analysis.get('age', 25)
            gender = analysis.get('dominant_gender', 'unknown')
            
            # Limpiar archivo temporal
            try:
                os.remove(temp_path)
            except:
                pass
            
            return {
                'emotion': dominant_emotion,
                'confidence': emotion_confidence,
                'age': age,
                'gender': gender,
                'all_emotions': emotions
            }
    except Exception as e:
        print(f"Error en anÃ¡lisis: {e}")
        
    return None

def draw_results_on_frame(frame, results):
    """Dibuja los resultados en el frame."""
    if not results:
        return frame
    
    # Detectar rostros
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, 1.1, 4)
    
    # Mapeo de emociones a emojis y colores
    emotion_emojis = {
        'angry': 'ğŸ˜ ', 'disgust': 'ğŸ¤¢', 'fear': 'ğŸ˜¨',
        'happy': 'ğŸ˜€', 'sad': 'ğŸ˜¢', 'surprise': 'ğŸ˜®', 'neutral': 'ğŸ˜'
    }
    
    colors = {
        'angry': (0, 0, 255), 'disgust': (0, 255, 0), 'fear': (255, 0, 255),
        'happy': (0, 255, 255), 'sad': (255, 0, 0), 'surprise': (255, 165, 0),
        'neutral': (128, 128, 128)
    }
    
    for (x, y, w, h) in faces:
        emotion = results['emotion']
        color = colors.get(emotion, (255, 255, 255))
        emoji = emotion_emojis.get(emotion, 'â“')
        
        # Dibujar rectÃ¡ngulo
        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 3)
        
        # Preparar texto
        emotion_text = f"{emoji} {emotion.upper()}"
        age_gender_text = f"Age: {results['age']} | {results['gender'].title()}"
        confidence_text = f"Conf: {results['confidence']:.2f}"
        
        # Configurar fuente
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.7
        thickness = 2
        
        # Obtener tamaÃ±os de texto
        (emotion_width, emotion_height), _ = cv2.getTextSize(emotion_text, font, font_scale, thickness)
        (age_width, age_height), _ = cv2.getTextSize(age_gender_text, font, 0.5, 1)
        
        # Dibujar fondo
        cv2.rectangle(frame, (x, y - emotion_height - 80), (x + max(emotion_width, age_width) + 15, y), color, -1)
        
        # Dibujar textos
        cv2.putText(frame, emotion_text, (x + 5, y - 55), font, font_scale, (255, 255, 255), thickness)
        cv2.putText(frame, age_gender_text, (x + 5, y - 35), font, 0.5, (255, 255, 255), 1)
        cv2.putText(frame, confidence_text, (x + 5, y - 15), font, 0.5, (255, 255, 255), 1)
    
    return frame

def create_emotion_chart(emotion_history):
    """Crea grÃ¡fico de emociones."""
    if not emotion_history:
        return None
    
    emotion_counts = defaultdict(int)
    for emotion in emotion_history:
        emotion_counts[emotion] += 1
    
    fig = px.bar(
        x=list(emotion_counts.keys()),
        y=list(emotion_counts.values()),
        title="EvoluciÃ³n de Emociones",
        labels={'x': 'Emociones', 'y': 'Frecuencia'},
        color=list(emotion_counts.keys()),
        color_discrete_map={
            'angry': '#FF0000', 'disgust': '#00FF00', 'fear': '#FF00FF',
            'happy': '#FFFF00', 'sad': '#0000FF', 'surprise': '#FFA500',
            'neutral': '#808080'
        }
    )
    
    fig.update_layout(showlegend=False, height=400, margin=dict(l=20, r=20, t=40, b=20))
    return fig

def main():
    st.title("ğŸ§  FaceMood AI - Simple")
    st.markdown("### DetecciÃ³n de emociones en tiempo real - VersiÃ³n optimizada")
    
    # Inicializar session state
    if 'emotion_history' not in st.session_state:
        st.session_state.emotion_history = []
    if 'last_analysis' not in st.session_state:
        st.session_state.last_analysis = None
    if 'analysis_count' not in st.session_state:
        st.session_state.analysis_count = 0
    if 'running' not in st.session_state:
        st.session_state.running = False
    
    # Sidebar con configuraciÃ³n
    st.sidebar.title("âš™ï¸ ConfiguraciÃ³n")
    analysis_interval = st.sidebar.slider(
        "Frecuencia de anÃ¡lisis (frames)", 
        min_value=10, 
        max_value=50, 
        value=15,
        help="Menor valor = mÃ¡s frecuente (pero mÃ¡s lento)"
    )
    
    cooldown_time = st.sidebar.slider(
        "Tiempo entre anÃ¡lisis (segundos)", 
        min_value=1.0, 
        max_value=5.0, 
        value=1.5,
        step=0.5,
        help="Menor valor = mÃ¡s anÃ¡lisis por minuto"
    )
    
    # Layout principal
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ğŸ“¹ CÃ¡mara Web")
        
        # Placeholder para la imagen
        image_placeholder = st.empty()
        
        # Controles
        start_button = st.button("ğŸ¥ Iniciar CÃ¡mara", type="primary")
        stop_button = st.button("â¹ï¸ Detener")
        
        if start_button:
            st.session_state.running = True
        
        if stop_button:
            st.session_state.running = False
        
        # Mostrar estado
        if st.session_state.running:
            st.success("âœ… CÃ¡mara activa - Analizando emociones...")
        else:
            st.info("ğŸ“± Presiona 'Iniciar CÃ¡mara' para comenzar")
        
        # Captura de video
        if st.session_state.running:
            cap = cv2.VideoCapture(0)
            
            if cap.isOpened():
                # Configurar anÃ¡lisis
                frame_count = 0
                last_analysis_time = 0
                current_results = None
                
                # Loop principal
                while st.session_state.running:
                    ret, frame = cap.read()
                    if not ret:
                        break
                    
                    # Voltear frame
                    frame = cv2.flip(frame, 1)
                    
                    # Analizar cada cierto nÃºmero de frames (MEJORADO)
                    current_time = time.time()
                    if (frame_count % analysis_interval == 0 and 
                        current_time - last_analysis_time > cooldown_time):
                        
                        # Analizar frame
                        results = analyze_frame(frame)
                        if results:
                            current_results = results
                            st.session_state.last_analysis = results
                            st.session_state.analysis_count += 1
                            st.session_state.emotion_history.append(results['emotion'])
                            
                            # Mantener solo Ãºltimos 30 (aumentado)
                            if len(st.session_state.emotion_history) > 30:
                                st.session_state.emotion_history.pop(0)
                            
                            last_analysis_time = current_time
                            print(f"ğŸ­ EmociÃ³n: {results['emotion']} ({results['confidence']:.2f}) - Edad: {results['age']} - GÃ©nero: {results['gender']}")
                    
                    # Dibujar resultados en frame
                    if current_results:
                        frame = draw_results_on_frame(frame, current_results)
                    
                    # Convertir BGR a RGB para Streamlit
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    
                    # Mostrar frame (CORREGIDO: use_container_width)
                    image_placeholder.image(frame_rgb, channels="RGB", use_container_width=True)
                    
                    frame_count += 1
                    
                    # Control de FPS mejorado
                    time.sleep(0.025)  # ~40 FPS
                
                cap.release()
            else:
                st.error("âŒ No se pudo acceder a la cÃ¡mara")
                st.session_state.running = False
    
    with col2:
        st.subheader("ğŸ“Š Resultados en Tiempo Real")
        
        # Mostrar Ãºltimo anÃ¡lisis
        if st.session_state.last_analysis:
            results = st.session_state.last_analysis
            
            # Mapeo de emojis
            emotion_emojis = {
                'angry': 'ğŸ˜ ', 'disgust': 'ğŸ¤¢', 'fear': 'ğŸ˜¨',
                'happy': 'ğŸ˜€', 'sad': 'ğŸ˜¢', 'surprise': 'ğŸ˜®', 'neutral': 'ğŸ˜'
            }
            
            # MÃ©tricas en columnas
            col_met1, col_met2 = st.columns(2)
            
            with col_met1:
                st.metric("EmociÃ³n", 
                         f"{emotion_emojis.get(results['emotion'], 'â“')} {results['emotion'].title()}")
                st.metric("Confianza", f"{results['confidence']:.1%}")
            
            with col_met2:
                st.metric("Edad", f"{results['age']} aÃ±os")
                st.metric("GÃ©nero", results['gender'].title())
            
            st.metric("AnÃ¡lisis Realizados", st.session_state.analysis_count)
            
            # InformaciÃ³n detallada
            st.info(f"ğŸ­ **Ãšltima emociÃ³n**: {emotion_emojis.get(results['emotion'], 'â“')} {results['emotion'].title()} ({results['confidence']:.1%})")
        
        # GrÃ¡fico de emociones
        st.subheader("ğŸ“ˆ EstadÃ­sticas")
        
        if st.session_state.emotion_history:
            fig = create_emotion_chart(st.session_state.emotion_history)
            if fig:
                # CORREGIDO: use_container_width en lugar de use_column_width
                st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("ğŸ“Š Los grÃ¡ficos aparecerÃ¡n cuando se detecten emociones")
        
        # Controles
        st.subheader("ğŸ”§ Controles")
        
        if st.button("ğŸ”„ Resetear EstadÃ­sticas"):
            st.session_state.emotion_history = []
            st.session_state.last_analysis = None
            st.session_state.analysis_count = 0
            st.rerun()
        
        # InformaciÃ³n
        st.subheader("â„¹ï¸ InformaciÃ³n")
        st.info(f"""
        **ConfiguraciÃ³n actual:**
        - AnÃ¡lisis cada {analysis_interval} frames
        - Cooldown: {cooldown_time}s entre anÃ¡lisis
        - CÃ¡mara a ~40 FPS
        - ActualizaciÃ³n en tiempo real
        """)

if __name__ == "__main__":
    main() 