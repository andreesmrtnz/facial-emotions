#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FaceMood AI - WebRTC Version - Detecci√≥n de emociones en tiempo real
====================================================================

Versi√≥n optimizada con WebRTC para acceso a c√°mara web desde navegador.
"""

import streamlit as st
import cv2
import numpy as np
import time
from datetime import datetime
import tempfile
import os
from deepface import DeepFace
import plotly.express as px
from collections import defaultdict
import threading
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase, RTCConfiguration
import av

# Configuraci√≥n de la p√°gina
st.set_page_config(
    page_title="FaceMood AI - WebRTC",
    page_icon="üß†",
    layout="wide"
)

# Configuraci√≥n RTC para WebRTC
RTC_CONFIGURATION = RTCConfiguration({
    "iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]
})

class FaceMoodTransformer(VideoTransformerBase):
    """Transformador de video para an√°lisis en tiempo real con WebRTC."""
    
    def __init__(self):
        self.frame_count = 0
        self.analysis_interval = 30  # Analizar cada 30 frames
        self.current_results = {
            'emotion': 'neutral',
            'age': 25,
            'gender': 'unknown',
            'confidence': 0.0
        }
        self.emotion_history = []
        self.lock = threading.Lock()
        self.last_analysis_time = 0
        self.analysis_cooldown = 3.0  # 3 segundos entre an√°lisis
        
        # Mapeo de emociones a emojis
        self.emotion_emojis = {
            'angry': 'üò†', 'disgust': 'ü§¢', 'fear': 'üò®',
            'happy': 'üòÄ', 'sad': 'üò¢', 'surprise': 'üòÆ', 'neutral': 'üòê'
        }
        
        # Colores para emociones (BGR)
        self.colors = {
            'angry': (0, 0, 255), 'disgust': (0, 255, 0), 'fear': (255, 0, 255),
            'happy': (0, 255, 255), 'sad': (255, 0, 0), 'surprise': (255, 165, 0),
            'neutral': (128, 128, 128)
        }
    
    def recv(self, frame):
        """Procesa cada frame del video."""
        img = frame.to_ndarray(format="bgr24")
        current_time = time.time()
        
        # Analizar cada cierto n√∫mero de frames y con cooldown
        if (self.frame_count % self.analysis_interval == 0 and 
            current_time - self.last_analysis_time > self.analysis_cooldown):
            
            self.last_analysis_time = current_time
            
            try:
                # Crear archivo temporal con nombre √∫nico
                temp_filename = f"temp_frame_{int(current_time * 1000)}.jpg"
                temp_path = os.path.join(tempfile.gettempdir(), temp_filename)
                
                # Guardar frame
                cv2.imwrite(temp_path, img)
                
                # Analizar con DeepFace
                result = DeepFace.analyze(
                    temp_path, 
                    actions=['emotion', 'age', 'gender'],
                    enforce_detection=False
                )
                
                if result and len(result) > 0:
                    analysis = result[0]
                    
                    # Extraer datos
                    emotions = analysis.get('emotion', {})
                    dominant_emotion = max(emotions, key=emotions.get) if emotions else 'neutral'
                    emotion_confidence = emotions.get(dominant_emotion, 0) / 100.0
                    age = analysis.get('age', 25)
                    gender = analysis.get('dominant_gender', 'unknown')
                    
                    # Actualizar resultados
                    with self.lock:
                        self.current_results = {
                            'emotion': dominant_emotion,
                            'confidence': emotion_confidence,
                            'age': age,
                            'gender': gender,
                            'all_emotions': emotions
                        }
                        
                        # Agregar a historial
                        self.emotion_history.append({
                            'time': current_time,
                            'emotion': dominant_emotion,
                            'confidence': emotion_confidence
                        })
                        
                        # Mantener solo los √∫ltimos 50 registros
                        if len(self.emotion_history) > 50:
                            self.emotion_history.pop(0)
                
                # Limpiar archivo temporal
                try:
                    os.remove(temp_path)
                except:
                    pass
                    
            except Exception as e:
                print(f"Error en an√°lisis: {e}")
        
        self.frame_count += 1
        
        # Dibujar resultados en el frame
        img = self.draw_results_on_frame(img)
        
        return av.VideoFrame.from_ndarray(img, format="bgr24")
    
    def draw_results_on_frame(self, img):
        """Dibuja los resultados en el frame."""
        with self.lock:
            results = self.current_results
        
        if not results:
            return img
        
        # Detectar rostros
        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(gray, 1.1, 4)
        
        for (x, y, w, h) in faces:
            emotion = results['emotion']
            color = self.colors.get(emotion, (255, 255, 255))
            emoji = self.emotion_emojis.get(emotion, '‚ùì')
            
            # Dibujar rect√°ngulo alrededor del rostro
            cv2.rectangle(img, (x, y), (x+w, y+h), color, 2)
            
            # Agregar texto con emoci√≥n
            text = f"{emoji} {emotion.upper()}"
            cv2.putText(img, text, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
            
            # Agregar edad y g√©nero
            age_text = f"Age: {results['age']}"
            gender_text = f"Gender: {results['gender']}"
            cv2.putText(img, age_text, (x, y+h+20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
            cv2.putText(img, gender_text, (x, y+h+40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
        
        return img

def create_emotion_chart(emotion_history):
    """Crea un gr√°fico de las emociones detectadas."""
    if not emotion_history:
        return None
    
    # Contar emociones
    emotion_counts = defaultdict(int)
    for entry in emotion_history:
        emotion_counts[entry['emotion']] += 1
    
    if not emotion_counts:
        return None
    
    # Crear gr√°fico
    fig = px.bar(
        x=list(emotion_counts.keys()),
        y=list(emotion_counts.values()),
        title="Emociones Detectadas",
        labels={'x': 'Emoci√≥n', 'y': 'Frecuencia'},
        color=list(emotion_counts.values()),
        color_continuous_scale='viridis'
    )
    
    fig.update_layout(
        xaxis_title="Emoci√≥n",
        yaxis_title="Frecuencia",
        showlegend=False
    )
    
    return fig

def main():
    """Funci√≥n principal de la aplicaci√≥n."""
    
    # T√≠tulo y descripci√≥n
    st.title("üß† FaceMood AI - WebRTC")
    st.markdown("**Descubre tu emoci√≥n, edad y g√©nero en tiempo real con inteligencia artificial**")
    
    # Sidebar con informaci√≥n
    with st.sidebar:
        st.header("‚ÑπÔ∏è Informaci√≥n")
        st.markdown("""
        **C√≥mo usar:**
        1. Haz clic en "START" para activar la c√°mara
        2. Permite acceso a tu c√°mara cuando el navegador lo solicite
        3. Mira a la c√°mara y cambia expresiones faciales
        4. Observa los resultados en tiempo real
        
        **Emociones detectadas:**
        - üòÄ Feliz (Happy)
        - üò¢ Triste (Sad)
        - üò† Enfadado (Angry)
        - üòÆ Sorprendido (Surprise)
        - üò® Miedo (Fear)
        - ü§¢ Asco (Disgust)
        - üòê Neutral (Neutral)
        """)
        
        st.header("‚öôÔ∏è Configuraci√≥n")
        st.info("Esta versi√≥n usa WebRTC para acceso directo a tu c√°mara web desde el navegador.")
    
    # Contenedor principal
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.header("üìπ C√°mara en Tiempo Real")
        
        # WebRTC Streamer
        webrtc_ctx = webrtc_streamer(
            key="facemood",
            video_transformer_factory=FaceMoodTransformer,
            rtc_configuration=RTC_CONFIGURATION,
            media_stream_constraints={"video": True, "audio": False},
            async_processing=True,
        )
        
        if webrtc_ctx.state.playing:
            st.success("‚úÖ C√°mara activa - Analizando emociones...")
        else:
            st.info("üëÜ Haz clic en 'START' para activar la c√°mara")
    
    with col2:
        st.header("üìä Resultados")
        
        if webrtc_ctx.state.playing and webrtc_ctx.video_transformer:
            transformer = webrtc_ctx.video_transformer
            
            with transformer.lock:
                results = transformer.current_results
                emotion_history = transformer.emotion_history
            
            # Mostrar resultados actuales
            if results:
                emotion = results['emotion']
                emoji = transformer.emotion_emojis.get(emotion, '‚ùì')
                
                st.metric(
                    label="Emoci√≥n Detectada",
                    value=f"{emoji} {emotion.upper()}",
                    delta=f"{results['confidence']:.1%} confianza"
                )
                
                st.metric(
                    label="Edad Estimada",
                    value=f"{results['age']} a√±os"
                )
                
                st.metric(
                    label="G√©nero",
                    value=results['gender'].upper()
                )
                
                # Mostrar todas las emociones
                if 'all_emotions' in results:
                    st.subheader("üìà Todas las Emociones")
                    emotions_df = pd.DataFrame(
                        list(results['all_emotions'].items()),
                        columns=['Emoci√≥n', 'Confianza']
                    )
                    emotions_df['Confianza'] = emotions_df['Confianza'].apply(lambda x: f"{x:.1f}%")
                    st.dataframe(emotions_df, use_container_width=True)
                
                # Gr√°fico de historial
                if emotion_history:
                    st.subheader("üìä Historial de Emociones")
                    chart = create_emotion_chart(emotion_history)
                    if chart:
                        st.plotly_chart(chart, use_container_width=True)
            else:
                st.info("üëÄ Esperando detecci√≥n de rostro...")
        else:
            st.info("üé• Activa la c√°mara para ver resultados")
    
    # Footer
    st.markdown("---")
    st.markdown(
        """
        <div style='text-align: center; color: #666;'>
        <p>üß† FaceMood AI - Desarrollado con DeepFace y Streamlit</p>
        <p>üí° Esta versi√≥n usa WebRTC para acceso directo a la c√°mara web</p>
        </div>
        """,
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    import pandas as pd
    main() 